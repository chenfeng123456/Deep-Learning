\documentclass{article}

%文献引用，标准类型为plain
%\usepackage[hyperref=true,backend=biber,sorting=none,backref=true]{biblatex}
%\addbibresource{ref.bib}
\bibliographystyle{plain}
\usepackage{cite}


%超链接
\usepackage[pdftex,linkcolor=yellow,citecolor=red,backref=page]{hyperref}
\hypersetup{
bookmarks=true,
colorlinks=true,
linkcolor=blue
}


%字体颜色
\usepackage{xcolor}


%添加首行缩进
\usepackage{indentfirst}
\setlength{\parindent}{2em}


%改变页边距
\usepackage[a4paper,left=20mm,right=20mm,top=10mm,bottom=10mm]{geometry}

\title{Literature Review}
\author{Guorui Lu}
\date{2018.8.15}
\begin{document}
\maketitle

\section{History and Significance}
 
\indent Synthesizing realistic photo from sketches drawn by human has been a challenging and hot problem for a long time. The research of this technique can be traced back to 2009\cite{Chen2009Sketch2Photo}. However, the technique of image generation and image-to-image transfer did not rise up until 2014, when the Generative Adversarial Nets(GAN)\cite{Goodfellow2014Generative} was proposed. Since then, variety of methods have been proposed to generate a more realistic photo for its broad application prospect. For example, police can use it to catch suspects through synthesised photos based on the drawn sketches, which requires that our photo should be as accurate as possible. As for the entertainment industry, we may need the model be able to generate images of multiple style, which is also a popular but difficult task.

\section{Mainstream Method}
\indent I will introduce the main method of this project from 2014 to the present because the actual reasearch on sketch-photo synthesis did't qppear until 2014, which I think is largely due to the fact that GAN \cite{Goodfellow2014Generative} was present in 2014. Before it, sketches were mainly used to do retrieval work \cite{Cao2010MindFinder, Eitz2010An, Hu2010Gradient, Wang2010MindFinder, Cao2011Edgel, Hu2011A, Hu2013A, Lin20143D}.


\subsection{Generative Adversarial Nets($GAN$)}
Generative Adversarial Nets and its thoughts from game theory is the most popular and useful method in image generation. Here, I would clarify the GANs' development and show some recent work which can reflect the use of GAN in our project.




\subsubsection{Generative Adversarial Nets}
\indent The original GAN brought us a completely new idea. It is corresponding a minimax two-player game, which is simple and efficient. But it also has an obvious shortcoming that the constraint is too week so that we can't control what it will generate.

\subsubsection{cGAN and InfoGAN}
\indent Soon after the GAN's appearance, conditional GAN \cite{Mirza2014Conditional} was proposed, and then the InfoGAN \cite{Chen2016InfoGAN}. These two models are able to partly control the outputs by adding extra latent codes. However, the pictures they synthesize are still blurry and low-resolution.


\subsubsection{Pix2pix and Pix2pixHD}
\indent Pix2pix \cite{Isola2017Image} and pix2pixHD \cite{pix2pixHD} are two of the improved versions of the GANs. They took a big step forward to addressing the resolution issues. What they didn't resolve is that only when we have lots of paired images can we train the models.

\subsubsection{CycleGAN}
\indent Inspired by NLP, Jun-Yan Zhu and Taesung Park et al. came up with a new model named CycleGan \cite{Zhu2017Unpaired}. Although it has some drawbacks such as the high computing cost, CycleGAN still works very well for many problems. So I think I can apply it to my project.

\subsubsection{\underline{\textbf{\emph{Rescent Work}}}}
\indent Here I would show you some rescent work of sketch-photo in 2016, 2017 and 2018.
\begin{itemize}
\item \textbf{SketchyGAN: Towards Diverse and Realistic Sketch to Image Synthesis\cite{Chen2018SketchyGAN}}: \par
A new network structure proposed for generative task. It works better than retrieval model but the result is still blurry and low-resolution. Moreover, it is too faithful to the badly drawn pictures to keep the realism.

\item \textbf{Facial Attributes Guided Deep Sketch-to-Photo Synthesis\cite{Kazemi2018Facial}}: \par
This paper puts forward a method that add an auxiliary attribute discriminator to find the false attributes in the output of the generator. However, editing an attribute can cause unwanted structural edition of the image in some area, which is also the weakness of the previous models.

\item \textbf{TextureGAN: Controlling Deep Image Synthesis with Texture Patches \cite{Xian2017TextureGAN}}: \par
Just as its name implies, TextureGAN adds a texture patch on a sketch at arbitrary locations and scales, which allows it to synthesize high-quality photos. And its novel losses for training deep image synthesis can encourage the generative network to handle new textures never seen on existing objects.

\item \textbf{Scribbler: Controlling Deep Image Synthesis with Sketch and Color\cite{Sangkloy2016Scribbler}}: \par
In this research, Pastsorn Sangkloy et al. use sketched with sparse color "scribbles" descided by human to generate a higher resolution and more diverse images. However, if users specifies uncommon or even wrong colors and shapes, this model would deem them correct. In addition, sometimes the boundaries between object parts or regions of defferent colors are likely to become vague.


\item \textbf{Image Generation from Sketch Constraint Using Contextual GAN\cite{Lu2017Image}}: \par
This is the paper I appreciate most for it proposed a creative idea that we can see the generation of image based on sketches as a inpainting process so that the model can avoid being too strictly corresponding to the input paintings. 


\end{itemize}


\subsection{Deep Convolutional Neural Networks($DNNs$)}
\indent Although GANs has been a remarkable success in image generation field, deep convolutional neural network is still an effective tool. However, the use of this method is obviously less than GANs because it is difficult for a user to control what the network produce, which limits the application in image generation field.



\begin{itemize}
\item \textbf{Convolutional Sketch Inversion \cite{G2016Convolutional}}: \par
Unlike GANs, this model can synthesize realistic photos from sketches without extra constraint. But the output is still low-resolution.

\item \textbf{Attribute-controlled face photo synthesis from simple line drawing\cite{Guo2017Attribute}}:\par
This model can generate photo with desired attribute even  though
the simple line drawing is not complete. However, the style is random and a style photo is needed if we want to control the style of synthsized photo.


\end{itemize}


\section{Other Related Work}
\indent Apart from the above work that relate to sketch-photo synthesis directly, there are some other researches which are associated with the project.




\begin{table}[htbp]
\centering
\begin{tabular}{|p{1cm}|p{4.5cm}|p{5.5cm}|p{5.5cm}|}
\hline
time&title&contribution&limitation\\

\hline
2016&Sketch me that shoe\cite{Q_Yu2016Sketch}&Solve the problem of fine-grained instance-level SBIR using free-hand sketches and contribute two new
fine-grained SBIR datasets with extensive ground truth annotations & \\

\hline
2016&Colorful Image Colorization\cite{Zhang2016Colorful}&Introduce a novel framework for testing colorization algorithms, potentially applicable to other image synthesis tasks.& \\

\hline
2016&Let there be color!: joint end-to-end learning of global and local image priors for automatic image colorization with simultaneous classification\cite{Iizuka2016Let}&The proposed architecture can process images of any resolution.&The main limitation lies in the fact that it is data-driven and thus will only be able to colorize images that share common properties with those in the training set. The model will use the color it have learned while training despite outher colors may also be suitable. \\



\hline
2017&Real-Time User-Guided Image Colorization with Learned Deep Priors\cite{Efros2017Real}&It can colorize an image well with limitated time. And it can even generate unusual colorizations despite being trained on natural images.& It can be over-optimistic and produce undesired non-local effects\\

\hline


\end{tabular}
\end{table}


\section{\textbf{\underline{\emph{Focus Problems and My Ideas}}}}

\indent While reading papers, I found the main controversies constrate on the flollowing aspects:

\begin{enumerate}
\item \textbf{Lack of sketches, especially the paired images and sketches}\par
We can use CycleGAN to avoid this problem because CycleGAN doesn't need paired images.


\item \textbf{Uncontrollable results}\par
It seems difficult for models to learn the latent information that controls the results directly, so the main idea of solving this problem is that use extra restricts, such as colors and textures, to specify what we want to get from the sketch, which requires the restricts given by people must be real and correct. 


\item \textbf{Low-resolution}\par
Just like the previous problems, adding constraints makes it easier for us to improve the resolution. Experiments have shown that using sketches with color specified can synthsize more realistic and higher resolution photos, so I suspect that we can apply another model which is used to colorize the picture first, and then use the colorized sketch to synthesize the photo.

\item \textbf{Badly drawn pictures}\par
We can pose the image generation problem as an image completion problem by using sketch as a weak constextual constraint. By doing so, the generated image may exhibit different poses and shapes beyond the input sketches which may not strictly corresponding to photographic objects.




\item \textbf{\textcolor{cyan}{Models can't fit the objects that don't appear during the training procedure}} \par
I haven't found any solution from papers above. Maybe zero-shot learning can be helpful.

\end{enumerate}


\section{Status and Future Direction}
\indent Sketch-to-photo synthesis has reached a quite realistic level, but some intractable problems still exit, which I have refered in $Focus\ Problems\ and\ My\ Ideas$. To my surprise, image colorization has also reached an amazing effect, which gives me the inspiration that we can combine the colorization and generation to get better results.\par
So this is my thoughts: we can select a model which can colorize the sketch, based on which we use the generative model to synthesize the realistic and high-resolution photo.
But there is a final problem still remained: how can the model deal with the situation tha doesn't appear during the training procedure? I'll continue to  study in depth. 

%\begin{thebibliography}{}
%\addtolength{\itemsep}{-1.5ex}
%\bibitem [1]{Sketch2photo} Sketch2Photo T.Cao
%\end{thebibliography}

\bibliography{ref.bib}

%----处理参考文献的新方法
%\printbibliography

\end{document}